{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOns2c2B9vsWQ+khIJlOKYa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f5088dba865649a7a1568ad5b1ce3c12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61deef5e28534fd8927178068ef6542b","IPY_MODEL_8d8947a21c2a4e02809adf19934531ca","IPY_MODEL_b480c4a12a7b451ba128aab8e10b1fee"],"layout":"IPY_MODEL_16053761c6d946aa8a1f575b6b4d3932"}},"61deef5e28534fd8927178068ef6542b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57b8c86c12aa4a5cb46f79be2b39fbac","placeholder":"​","style":"IPY_MODEL_e2ca0b23f9634b3b889a799384f5a73e","value":"Map: 100%"}},"8d8947a21c2a4e02809adf19934531ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e337624e5c9449caf56bde39667b6fb","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b069df6a7291433fb14c8b2e453c91a7","value":100}},"b480c4a12a7b451ba128aab8e10b1fee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_035f135d1fd34dceb43c98fd6d1ccf9d","placeholder":"​","style":"IPY_MODEL_266cb53f06d64f489ae7a591c7bbcfa9","value":" 100/100 [00:00&lt;00:00, 871.05 examples/s]"}},"16053761c6d946aa8a1f575b6b4d3932":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57b8c86c12aa4a5cb46f79be2b39fbac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2ca0b23f9634b3b889a799384f5a73e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e337624e5c9449caf56bde39667b6fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b069df6a7291433fb14c8b2e453c91a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"035f135d1fd34dceb43c98fd6d1ccf9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"266cb53f06d64f489ae7a591c7bbcfa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q --upgrade transformers datasets peft accelerate bitsandbytes sentencepiece\n"],"metadata":{"id":"D_UxHALaT77D","executionInfo":{"status":"ok","timestamp":1755856574373,"user_tz":-600,"elapsed":5820,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"gretelai/symptom_to_diagnosis\")\n","print(ds.keys())\n","print(len(ds[\"train\"]))\n","print(ds[\"train\"][0])\n","\n","import json\n","\n","# Load the training split of the dataset into a list of dictionaries\n","file = ds[\"train\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tLVSEACTrVX","executionInfo":{"status":"ok","timestamp":1755856576758,"user_tz":-600,"elapsed":2376,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}},"outputId":"a304eaa5-c288-4d27-f206-6895d000e28b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['train', 'test'])\n","853\n","{'output_text': 'cervical spondylosis', 'input_text': \"I've been having a lot of pain in my neck and back. I've also been having trouble with my balance and coordination. I've been coughing a lot and my limbs feel weak.\"}\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Mh01O2aITlHR","executionInfo":{"status":"ok","timestamp":1755856576761,"user_tz":-600,"elapsed":1,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}}},"outputs":[],"source":["def reformat_data(example):\n","  \"\"\"Reformats a dataset example into the desired JSON structure.\"\"\"\n","  return {\n","      \"instruction\": example[\"input_text\"],\n","      \"output\": example[\"output_text\"]\n","  }\n","\n","# Apply the reformatting function to the training split and remove original columns\n","reformatted_train_data = ds[\"train\"].map(reformat_data, remove_columns=[\"input_text\", \"output_text\"])"]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mL003whgacJT","executionInfo":{"status":"ok","timestamp":1755856576765,"user_tz":-600,"elapsed":3,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}},"outputId":"33dedabe-0924-49bc-db0a-5c5ac96ad250"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["4.55.3\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","from datasets import load_dataset, Dataset\n","import numpy as np # Import numpy for np.where in the evaluation cell\n","import torch # Import torch\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n","\n","# Load and reformat the dataset (assuming this is done in previous cells)\n","# If not, you would need to add the code from those cells here as well.\n","# For now, assuming 'reformatted_train_data' and 'reformatted_test_data' are available.\n","\n","# Tokenize the dataset\n","def tokenize_function(examples):\n","    inputs = examples[\"instruction\"] # inputs is a single string when batched=False\n","    # Tokenize the input string and return tensors\n","    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","\n","    # Tokenize labels (output string) and return tensors\n","    labels = tokenizer(\n","        text_target=examples[\"output\"],\n","        max_length=8,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","\n","    # When batched=False in map, tokenizer returns BatchEncoding with tensor values (shape [1, sequence_length]).\n","    # We need to remove the batch dimension [1].\n","    input_ids = model_inputs[\"input_ids\"].squeeze(0)\n","    attention_mask = model_inputs[\"attention_mask\"].squeeze(0)\n","    labels_input_ids = labels[\"input_ids\"].squeeze(0)\n","\n","\n","    # Replace pad tokens in labels with -100 so loss ignores them\n","    # This needs to be done on a tensor.\n","    labels_with_ignored_padding = torch.where(labels_input_ids == tokenizer.pad_token_id, torch.tensor(-100, dtype=torch.long), labels_input_ids)\n","\n","    # Return a dictionary with tensor values (shape [sequence_length]).\n","    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels_with_ignored_padding}\n","\n","\n","# Tokenize the datasets (assuming reformatted_train_data and reformatted_test_data are defined)\n","# If not defined, you need to run the cells that define them first.\n","# For the purpose of fixing the NameError, I will add the code to load and reformat the data as well to make this cell self-contained for demonstration.\n","\n","ds = load_dataset(\"gretelai/symptom_to_diagnosis\")\n","\n","def reformat_data(example):\n","  \"\"\"Reformats a dataset example into the desired JSON structure.\"\"\"\n","  return {\n","      \"instruction\": example[\"input_text\"],\n","      \"output\": example[\"output_text\"]\n","  }\n","\n","# Apply the reformatting function to the training split and remove original columns\n","reformatted_train_data = ds[\"train\"].map(reformat_data, remove_columns=[\"input_text\", \"output_text\"])\n","\n","# Create a validation split using select and then apply the reformatting function\n","ds[\"validation\"] = ds[\"test\"].select(range(100))\n","reformatted_test_data = ds[\"validation\"].map(reformat_data, remove_columns=[\"input_text\", \"output_text\"])\n","\n","\n","# Ensure batched=False here\n","tokenized_train_datasets = reformatted_train_data.map(tokenize_function, batched=False)\n","tokenized_test_datasets = reformatted_test_data.map(tokenize_function, batched=False)\n","\n","\n","# Define the data collator\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./flan-t5-lora-samsum\",\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    learning_rate=1e-5,\n","    num_train_epochs=1,\n","    logging_dir=\"./logs\",\n","    logging_steps=10,\n","    save_total_limit=2,\n","    eval_strategy=\"steps\",  # ✅ correct for 4.55.2\n","    eval_steps=50,\n","    save_steps=100,\n","    fp16=False,\n","    report_to=\"none\",\n",")\n","\n","\n","\n","# Initialize the trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_datasets,\n","    eval_dataset=tokenized_test_datasets, # Add an evaluation dataset if you split your data\n","    #train_dataset=reformatted_train_data,\n","    #eval_dataset=reformatted_test_data,\n","    #tokenizer=tokenizer,\n","    processing_class=tokenizer,\n","    data_collator=data_collator,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f5088dba865649a7a1568ad5b1ce3c12","61deef5e28534fd8927178068ef6542b","8d8947a21c2a4e02809adf19934531ca","b480c4a12a7b451ba128aab8e10b1fee","16053761c6d946aa8a1f575b6b4d3932","57b8c86c12aa4a5cb46f79be2b39fbac","e2ca0b23f9634b3b889a799384f5a73e","7e337624e5c9449caf56bde39667b6fb","b069df6a7291433fb14c8b2e453c91a7","035f135d1fd34dceb43c98fd6d1ccf9d","266cb53f06d64f489ae7a591c7bbcfa9"]},"id":"KGTvhObReeSc","executionInfo":{"status":"ok","timestamp":1755856598816,"user_tz":-600,"elapsed":10562,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}},"outputId":"a00db096-fa3e-44b4-a727-0d35cbc27d95"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5088dba865649a7a1568ad5b1ce3c12"}},"metadata":{}}]},{"cell_type":"code","source":["# Start training\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"XviJ7CNEehER","executionInfo":{"status":"ok","timestamp":1755856697492,"user_tz":-600,"elapsed":98359,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}},"outputId":"2454c790-6fda-420c-93ca-ba2f189ab72d","collapsed":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='214' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [214/214 01:37, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>2.432300</td>\n","      <td>2.198553</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.889200</td>\n","      <td>1.712560</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.951500</td>\n","      <td>1.540012</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.822300</td>\n","      <td>1.480714</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=214, training_loss=2.248125965350142, metrics={'train_runtime': 98.304, 'train_samples_per_second': 8.677, 'train_steps_per_second': 2.177, 'total_flos': 584098021638144.0, 'train_loss': 2.248125965350142, 'epoch': 1.0})"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# -------------------------\n","# Evaluation\n","# -------------------------\n","from tqdm import tqdm # Import tqdm\n","import torch\n","from torch.utils.data import DataLoader\n","\n","predictions = []\n","references = []\n","\n","# Create a DataLoader for the test dataset subset\n","# Assuming `tokenized_test_datasets` is available from previous steps\n","# Limit to the first 100 examples as indicated in the print statement\n","test_dataset_subset = tokenized_test_datasets.select(range(100))\n","test_dataloader_subset = DataLoader(test_dataset_subset, batch_size=4, collate_fn=data_collator)\n","\n","\n","print(\"Starting evaluation on 100 random test examples...\")\n","for batch in tqdm(test_dataloader_subset):\n","    if torch.cuda.is_available():\n","        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n","\n","    with torch.no_grad():\n","        generated_tokens = model.generate(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            max_length=20  # match labels\n","        )\n","\n","    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","\n","    # Replace -100 in labels to decode\n","    labels = batch[\"labels\"].cpu().numpy()\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels.tolist(), skip_special_tokens=True)\n","\n","    predictions.extend(decoded_preds)\n","    references.extend(decoded_labels)\n","\n","print(\"Evaluation complete.\")\n","\n","# -------------------------\n","# Simple Accuracy\n","# -------------------------\n","correct_predictions = sum([1 for pred, label in zip(predictions, references) if pred.strip() == label.strip()])\n","accuracy = correct_predictions / len(references) if len(references) > 0 else 0\n","print(f\"Accuracy on 100 random test examples: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"31ftVtWfFzQN","executionInfo":{"status":"error","timestamp":1755857636730,"user_tz":-600,"elapsed":26,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}},"outputId":"b931744a-f61e-478f-ab43-786351cdc17a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting evaluation on 100 random test examples...\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/25 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`instruction` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-932340158.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting evaluation on 100 random test examples...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;31m# run through tokenizer without labels to ensure no side effects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         batch = pad_without_fast_tokenizer_warning(\n\u001b[0m\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mnon_labels_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpad_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Restore the state of the warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3427\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     def create_token_type_ids_from_sequences(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    781\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m                     ) from e\n\u001b[0;32m--> 783\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`instruction` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."]}]},{"cell_type":"code","metadata":{"id":"54cd911f","executionInfo":{"status":"aborted","timestamp":1755856697581,"user_tz":-600,"elapsed":36,"user":{"displayName":"Pasindu Ambegoda","userId":"06530733606875410990"}}},"source":["# Inspect the first few examples of the tokenized training data\n","for i in range(3):\n","    print(f\"Example {i+1}:\")\n","    example = tokenized_train_datasets[i]\n","    print(\"Input IDs:\", example[\"input_ids\"][:20]) # Print first 20 tokens for brevity\n","    print(\"Labels:\", example[\"labels\"][:20]) # Print first 20 tokens for brevity\n","    # Optionally, decode to see the text representation\n","    print(\"Decoded Input:\", tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True))\n","    # Need to handle -100 in labels before decoding\n","    labels = example[\"labels\"].copy()\n","    labels = [label if label != -100 else tokenizer.pad_token_id for label in labels]\n","    print(\"Decoded Labels:\", tokenizer.decode(labels, skip_special_tokens=True))\n","    print(\"-\" * 30)"],"execution_count":null,"outputs":[]}]}